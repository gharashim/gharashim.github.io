---
layout: single
title:  "Temporal-Difference - Gridworld"
---

```python
import numpy as np
import matplotlib.pyplot as plt
from gridworld import GridworldEnv as Env
```

## 시각화


```python
def plot_square_grid_with_values(data):
    """
    Plots the given data as a square grid heatmap with values displayed on the grid.

    Args:
        data (numpy array): 1D array of data values.
        grid_size (int): Size of the square grid (grid_size x grid_size).
    """
    # Reshape array to a square grid
    grid_size = int(np.sqrt(len(data)))  # Assume it's a perfect square
    grid_data = data.reshape((grid_size, grid_size))
    
    # Plot as a heatmap
    plt.figure(figsize=(4, 4))
    plt.imshow(grid_data, cmap='coolwarm', interpolation='nearest')
    
    # Annotate grid with values
    for i in range(grid_size):
        for j in range(grid_size):
            plt.text(j, i, f"{grid_data[i, j]:.2f}", ha='center', va='center', color='black')
    
    # plt.title(f'{grid_size}x{grid_size} Grid Heatmap with Values')
    plt.xticks(range(grid_size))
    plt.yticks(range(grid_size))
    plt.grid(visible=False)
    plt.show()
```

# Monte - Carlo


```python
import numpy as np

class MCAgent:
    def __init__(self, env, gamma=0.99, epsilon=0.1):
        """
        Monte Carlo Agent
        :param env: The environment to interact with (e.g., GridworldEnv).
        :param gamma: Discount factor.
        :param epsilon: Exploration probability for epsilon-greedy policy.
        """
        self.env = env
        self.gamma = gamma
        self.epsilon = epsilon

        # Initialize value function and returns dictionary
        self.value_function = np.zeros(env.nS)
        self.returns = {state: [] for state in range(env.nS)}

    def policy(self, state):
        """
        Epsilon-greedy policy.
        :param state: Current state.
        :return: Action to take.
        """
        if np.random.rand() < self.epsilon:
            # Explore: Random action
            return self.env.action_space.sample()
        else:
            # Exploit: Greedy action based on the value function
            return self._best_action(state)

    def _best_action(self, state):
        """
        Finds the best action for a given state based on the value function.
        :param state: Current state.
        :return: Best action (int).
        """
        q_values = []
        for action in range(self.env.nA):
            transitions = self.env.P[state][action]
            q_value = sum(prob * (reward + self.gamma * self.value_function[next_state])
                          for prob, next_state, reward, _ in transitions)
            q_values.append(q_value)
        return np.argmax(q_values)

    def generate_episode(self):
        """
        Generate an episode by following the current policy.
        :return: List of (state, action, reward) tuples.
        """
        episode = []
        state = self.env.reset()
        done = False

        while not done:
            action = self.policy(state)
            next_state, reward, done, _ = self.env.step(action)
            episode.append((state, action, reward))
            state = next_state

        return episode

    def learn(self, episodes=1000):
        """
        Train the agent using the Monte Carlo algorithm.
        :param episodes: Number of episodes to train.
        """
        for _ in range(episodes):
            # Generate an episode
            episode = self.generate_episode()
            
            # Calculate returns for each state in the episode
            G = 0
            visited_states = set()
            for state, _, reward in reversed(episode):
                G = reward + self.gamma * G
                # Update value function only for the first visit
                if state not in visited_states:
                    visited_states.add(state)
                    self.returns[state].append(G)
                    self.value_function[state] = np.mean(self.returns[state])

    def evaluate_policy(self):
        """
        Evaluate the policy by calculating the value function.
        """
        print("Value Function:")
        for y in range(self.env.shape[0]):
            for x in range(self.env.shape[1]):
                state = y * self.env.shape[1] + x
                print(f"{self.value_function[state]:6.2f}", end=" ")
            print()

# Example usage
if __name__ == "__main__":
    env = Env(shape=[5, 5])  # Use the GridworldEnv class
    agent = MCAgent(env, gamma=0.99, epsilon=0.1)
    agent.learn(episodes=1000)
    agent.evaluate_policy()

```

    Value Function:
      0.00  -1.00  -2.47  -4.04  -5.82 
     -1.05  -2.45  -5.71  -7.17  -3.68 
     -2.21  -3.95  -9.39  -4.84  -2.21 
     -3.26  -5.13 -11.24  -4.31  -1.11 
     -6.91 -13.94  -2.28  -1.74   0.00 
    

# Temporal - Difference Method


```python
import numpy as np

class TDAgent:
    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=0.1):
        """
        TD(0) Agent
        :param env: The environment to interact with (e.g., GridworldEnv).
        :param alpha: Learning rate.
        :param gamma: Discount factor.
        :param epsilon: Exploration probability for epsilon-greedy policy.
        """
        self.env = env
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon

        # Initialize value function for each state
        self.value_function = np.zeros(env.nS)

    def policy(self, state):
        """
        Epsilon-greedy policy.
        :param state: Current state.
        :return: Action to take.
        """
        if np.random.rand() < self.epsilon:
            # Explore: Random action
            return self.env.action_space.sample()
        else:
            # Exploit: Greedy action based on the value function
            return self._best_action(state)

    def _best_action(self, state):
        """
        Finds the best action for a given state based on the value function.
        :param state: Current state.
        :return: Best action (int).
        """
        q_values = []
        for action in range(self.env.nA):
            transitions = self.env.P[state][action]
            q_value = sum(prob * (reward + self.gamma * self.value_function[next_state])
                          for prob, next_state, reward, _ in transitions)
            q_values.append(q_value)
        return np.argmax(q_values)

    def learn(self, episodes=1000):
        """
        Train the agent using the TD(0) algorithm.
        :param episodes: Number of episodes to train.
        """
        for episode in range(episodes):
            state = self.env.reset()
            done = False

            while not done:
                # Select action using policy
                action = self.policy(state)

                # Take action, observe next state and reward
                next_state, reward, done, _ = self.env.step(action)

                # TD(0) update rule
                td_target = reward + self.gamma * self.value_function[next_state] * (not done)
                td_error = td_target - self.value_function[state]
                self.value_function[state] += self.alpha * td_error

                # Update state
                state = next_state

    def evaluate_policy(self):
        """
        Evaluate the policy by calculating the value function.
        """
        print("Value Function:")
        for y in range(self.env.shape[0]):
            for x in range(self.env.shape[1]):
                state = y * self.env.shape[1] + x
                print(f"{self.value_function[state]:6.2f}", end=" ")
            print()

# Example usage
if __name__ == "__main__":
    env = Env(shape=[5, 5])
    agent = TDAgent(env, alpha=0.1, gamma=0.99, epsilon=0.1)
    agent.learn(episodes=1000)
    agent.evaluate_policy()

```

    Value Function:
      0.00  -1.05  -2.17  -3.41  -4.11 
     -1.17  -2.29  -3.27  -4.09  -3.38 
     -2.27  -3.45  -4.10  -3.17  -2.23 
     -3.40  -4.12  -3.12  -2.07  -1.15 
     -4.18  -3.23  -2.32  -1.05   0.00 
    


```python
import matplotlib.pyplot as plt

def calculate_true_value(env, gamma=0.99, threshold=1e-5):
    """
    Use Dynamic Programming (Bellman Equation) to calculate the true value function.
    """
    V = np.zeros(env.nS)
    while True:
        delta = 0
        for s in range(env.nS):
            v = V[s]
            V[s] = max(
                sum(prob * (reward + gamma * V[next_state])
                    for prob, next_state, reward, _ in env.P[s][a])
                for a in range(env.nA)
            )
            delta = max(delta, abs(v - V[s]))
        if delta < threshold:
            break
    return V

def plot_value_difference(env, agent, true_value, episodes=3000):
    """
    Train the agent and plot the variance of value differences over episodes.
    """
    value_differences = []

    for episode in range(episodes):
        # Train the agent for one episode
        agent.learn(episodes=1)
        
        # Calculate the difference and its variance
        difference = true_value - agent.value_function
        variance = np.var(difference)
        value_differences.append(variance)

    # Plot the variance over episodes
    plt.plot(range(1, episodes + 1), value_differences)
    plt.xlabel("Episodes")
    plt.ylabel("Variance of Value Difference")
    plt.grid()
    plt.show()

# Example usage
if __name__ == "__main__":
    # Initialize environment and agent
    env = Env(shape=[4, 4])
    agent1 = MCAgent(env, gamma=0.99, epsilon=0.1)
    agent2 = TDAgent(env, alpha=0.1, gamma=0.99, epsilon=0.1)

    # Calculate true value using DP
    true_value = calculate_true_value(env, gamma=0.99)

    # Plot the value difference variance
    
    plt.title("MC Agent vs True Value (DP) Variance")
    plot_value_difference(env, agent1, true_value, episodes=3000)
    
    plt.title("TD Agent vs True Value (DP) Variance")
    plot_value_difference(env, agent2, true_value, episodes=3000)

```


<img src="">


<img src="">
    


- Monte Carlo : 분산이 매우 크고 Bias가 적다.

- TD : 분산이 적으나 Bias(mean Error) 존재.
